print("TOKENISER\n");
install_root_thing();
install_identifiers();
init_tokeniser();

var old_filename_string_id: uint16 := 0;
var old_line_number: uint16 := 0;
var tokens_fd: uint8 := file_openout("tokens.dat");
var tokens_count: uint16 := 0;

sub write_token(token: uint16)
    var buf: uint16[1];
    buf[0] := token;
    tokens_count := tokens_count + 1;
    file_putblock(tokens_fd, &buf[0] as [int8], 2);
end sub;

while current_token != 0 loop
    if old_filename_string_id != current_filename_string_id then
        var filename_token_id: uint16 := add_string_thing(current_filename_string_id);
        write_token(TOKEN_FILENAME);
        write_token(filename_token_id);
        old_filename_string_id := current_filename_string_id;
    end if;

    if old_line_number != current_line_number then
        write_token(TOKEN_LINENUMBER);
        write_token(current_line_number);
        old_line_number := current_line_number;
    end if;

    write_token(current_token);
    next_token();
end loop;

print("tokens read: ");
print_i16(tokens_count);
print_newline();
file_close(tokens_fd);

fixup_strings();

print("string table size: ");
print_i16(stringtab_top - stringtab_base);
print_newline();
save_string_table("strings.dat");

print("thing table size: ");
print_i16(thing_table_top);
print_newline();
save_thing_table("things.dat");
